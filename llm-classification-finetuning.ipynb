{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":86518,"databundleVersionId":9809560,"sourceType":"competition"},{"sourceId":129073,"sourceType":"modelInstanceVersion","modelInstanceId":108753,"modelId":133071}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Kaggle Notebook for LLM Classification Finetuning Competition\n# Using BERT-base-uncased model for preference classification\n\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.optim import AdamW  # import AdamW from torch.optim now\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:14:48.306405Z","iopub.execute_input":"2025-05-19T11:14:48.306836Z","iopub.status.idle":"2025-05-19T11:14:50.223419Z","shell.execute_reply.started":"2025-05-19T11:14:48.306802Z","shell.execute_reply":"2025-05-19T11:14:50.222502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load train and test data\ntrain = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')\ntest = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')\n\nprint(f\"Train data shape: {train.shape}\")\nprint(f\"Test data shape: {test.shape}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:14:56.961116Z","iopub.execute_input":"2025-05-19T11:14:56.961445Z","iopub.status.idle":"2025-05-19T11:15:00.653643Z","shell.execute_reply.started":"2025-05-19T11:14:56.961422Z","shell.execute_reply":"2025-05-19T11:15:00.652660Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Prepare input text by combining prompt + responses for BERT input\ndef prepare_text(row):\n    return f\"Prompt: {row['prompt']} [SEP] Response A: {row['response_a']} [SEP] Response B: {row['response_b']}\"\n\ntrain['input_text'] = train.apply(prepare_text, axis=1)\ntest['input_text'] = test.apply(prepare_text, axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:15:01.850712Z","iopub.execute_input":"2025-05-19T11:15:01.851027Z","iopub.status.idle":"2025-05-19T11:15:02.584293Z","shell.execute_reply.started":"2025-05-19T11:15:01.851004Z","shell.execute_reply":"2025-05-19T11:15:02.583535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Convert winner columns into single label: 0 = A wins, 1 = B wins, 2 = Tie\ndef winner_to_label(row):\n    if row['winner_model_a'] == 1:\n        return 0\n    elif row['winner_model_b'] == 1:\n        return 1\n    else:\n        return 2\n\ntrain['label'] = train.apply(winner_to_label, axis=1)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:15:08.806763Z","iopub.execute_input":"2025-05-19T11:15:08.807049Z","iopub.status.idle":"2025-05-19T11:15:09.190586Z","shell.execute_reply.started":"2025-05-19T11:15:08.807030Z","shell.execute_reply":"2025-05-19T11:15:09.189893Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Load BERT-base-uncased tokenizer\n# 4. Load BERT-base-uncased tokenizer from local path\nfrom transformers import BertTokenizer\n\nMODEL_PATH = \"/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased\"\ntokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n\nMAX_LEN = 128\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:15:40.392971Z","iopub.execute_input":"2025-05-19T11:15:40.393565Z","iopub.status.idle":"2025-05-19T11:15:40.490645Z","shell.execute_reply.started":"2025-05-19T11:15:40.393535Z","shell.execute_reply":"2025-05-19T11:15:40.489749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Tokenize all text inputs for train and test\n# 5. Tokenize all text inputs for train and test\n#1. Use .apply() instead of tolist()\ndef tokenize_texts(texts, tokenizer, max_len=128):\n    return tokenizer(\n        list(texts),\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors='pt'\n    )\n#2. Limit Dataset for Testing\ntrain_encodings = tokenize_texts(train['input_text'], tokenizer, MAX_LEN)\ntest_encodings = tokenize_texts(test['input_text'], tokenizer, MAX_LEN)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:16:02.256173Z","iopub.execute_input":"2025-05-19T11:16:02.256943Z","iopub.status.idle":"2025-05-19T11:16:12.854742Z","shell.execute_reply.started":"2025-05-19T11:16:02.256916Z","shell.execute_reply":"2025-05-19T11:16:12.853830Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Create custom Dataset class for PyTorch\nclass LLMPreferenceDataset(Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.encodings['input_ids'])\n    \n    def __getitem__(self, idx):\n        item = {key: val[idx] for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_dataset = LLMPreferenceDataset(train_encodings, train['label'].values)\ntest_dataset = LLMPreferenceDataset(test_encodings)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:16:44.092282Z","iopub.execute_input":"2025-05-19T11:16:44.092648Z","iopub.status.idle":"2025-05-19T11:16:44.098959Z","shell.execute_reply.started":"2025-05-19T11:16:44.092592Z","shell.execute_reply":"2025-05-19T11:16:44.098261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Split train dataset into train and validation sets (80/20 split)\ntrain_size = int(0.8 * len(train_dataset))\nval_size = len(train_dataset) - train_size\n\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:17:04.063448Z","iopub.execute_input":"2025-05-19T11:17:04.063776Z","iopub.status.idle":"2025-05-19T11:17:04.070068Z","shell.execute_reply.started":"2025-05-19T11:17:04.063753Z","shell.execute_reply":"2025-05-19T11:17:04.069164Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. Load BERT-base-uncased model with 3 output labels (A wins, B wins, Tie)\n\nfrom transformers import BertForSequenceClassification\nfrom torch.optim import AdamW\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_path = \"/kaggle/input/bert-base-uncased/pytorch/default/1/bert-base-uncased\"\nfrom transformers import BertForSequenceClassification\n\nmodel = BertForSequenceClassification.from_pretrained(\n    model_path,\n    num_labels=9  # match the checkpoint for now\n)\n\n# Then replace the classifier with a new one (3 output labels)\nimport torch.nn as nn\nmodel.classifier = nn.Linear(model.config.hidden_size, 3)\nmodel.num_labels = 3\n\n#|model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:35:27.397760Z","iopub.execute_input":"2025-05-19T11:35:27.398033Z","iopub.status.idle":"2025-05-19T11:35:27.516439Z","shell.execute_reply.started":"2025-05-19T11:35:27.398015Z","shell.execute_reply":"2025-05-19T11:35:27.515770Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 9. Training and evaluation functions\ndef train_epoch(model, dataloader):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(dataloader):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef eval_model(model, dataloader):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            preds = outputs.logits.argmax(dim=1)\n\n            correct += (preds == labels).sum().item()\n            total += labels.size(0)\n    return correct / total\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T11:35:29.548454Z","iopub.execute_input":"2025-05-19T11:35:29.548768Z","iopub.status.idle":"2025-05-19T11:35:29.556566Z","shell.execute_reply.started":"2025-05-19T11:35:29.548746Z","shell.execute_reply":"2025-05-19T11:35:29.555655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10. Train model for 3 epochs\nfrom torch.optim import AdamW  # âœ… Use PyTorch AdamW to avoid import issues\n\n# Define the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# Number of training epochs\nEPOCHS = 3\n\n# Training loop\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n    \n    train_loss = train_epoch(model, train_loader)   # Make sure 'train_loader' is defined\n    val_acc = eval_model(model, val_loader)         # Make sure 'val_loader' is defined\n    \n    print(f\"Train loss: {train_loss:.4f}, Validation accuracy: {val_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:02:40.523309Z","iopub.execute_input":"2025-05-19T12:02:40.523660Z","iopub.status.idle":"2025-05-19T12:26:39.952898Z","shell.execute_reply.started":"2025-05-19T12:02:40.523636Z","shell.execute_reply":"2025-05-19T12:26:39.951740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 11. Predict probabilities on test set\ndef predict(model, dataloader):\n    model.eval()\n    preds = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask)\n            probs = torch.nn.functional.softmax(outputs.logits, dim=1)\n            preds.append(probs.cpu())\n    return torch.cat(preds)\n\ntest_loader = DataLoader(test_dataset, batch_size=32)\ntest_preds = predict(model, test_loader).numpy()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:27:11.954344Z","iopub.execute_input":"2025-05-19T12:27:11.954749Z","iopub.status.idle":"2025-05-19T12:27:12.515186Z","shell.execute_reply.started":"2025-05-19T12:27:11.954720Z","shell.execute_reply":"2025-05-19T12:27:12.514326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 12. Prepare submission file\nsubmission = pd.DataFrame({\n    'id': test['id'],\n    'winner_model_a': test_preds[:, 0],\n    'winner_model_b': test_preds[:, 1],\n    'winner_tie': test_preds[:, 2]\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:28:29.932346Z","iopub.execute_input":"2025-05-19T12:28:29.932692Z","iopub.status.idle":"2025-05-19T12:28:29.940714Z","shell.execute_reply.started":"2025-05-19T12:28:29.932666Z","shell.execute_reply":"2025-05-19T12:28:29.939813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}